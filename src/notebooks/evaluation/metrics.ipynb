{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.models.end_to_end import EndToEndSystem\n",
    "from src.plot_utils import dafx_from_name\n",
    "from src.dataset.paired_audio_dataset import PairedAudioDataset\n",
    "from src.callbacks.metrics import *\n",
    "from src.utils import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def get_val_checkpoint_filename(checkpoint_folder):\n",
    "    list_of_files = glob.glob(checkpoint_folder + \"/*.ckpt\")\n",
    "    val_file = [fl for fl in list_of_files if \"val\" in fl]\n",
    "    latest_file = max(val_file, key=os.path.getctime)\n",
    "    return latest_file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_checkpoint_for_effect(effect_name, checkpoints_dir):\n",
    "    checkpoint_id = effect_to_end_to_end_checkpoint_id(effect_name)\n",
    "    checkpoint_id_dir = os.path.join(checkpoints_dir, checkpoint_id + \"/checkpoints/\")\n",
    "    checkpoint_file = get_val_checkpoint_filename(checkpoint_id_dir)\n",
    "    return checkpoint_file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def get_results_filename(results_dir, dafx, dataset):\n",
    "    return results_dir + f\"/{dafx.split()[-1].lower()}_{dataset}.csv\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 24_000\n",
    "NUM_EXAMPLES = 1_000\n",
    "DAFX = \"mda Overdrive\"\n",
    "CHECKPOINTS_DIR = \"/home/kieran/Level5ProjectAudioVAE/src/train_scripts/l5proj_end2end\"\n",
    "AUDIO_DIR = \"/home/kieran/Level5ProjectAudioVAE/src/audio\"\n",
    "DATASET = \"daps\"\n",
    "DATASET_INPUT_DIRS = [f\"{DATASET}_{SAMPLE_RATE}\"]\n",
    "CHECKPOINT = get_checkpoint_for_effect(DAFX, CHECKPOINTS_DIR)\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "RESULTS_DIR = \"/home/kieran/Level5ProjectAudioVAE/src/notebooks/evaluation/data/metrics\"\n",
    "SEED = 1234"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pl.seed_everything(SEED)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"PESQ\": PESQ(SAMPLE_RATE),\n",
    "    \"MRSTFT\": auraloss.freq.MultiResolutionSTFTLoss(\n",
    "        fft_sizes=[32, 128, 512, 2048, 8192, 32768],\n",
    "        hop_sizes=[16, 64, 256, 1024, 4096, 16384],\n",
    "        win_lengths=[32, 128, 512, 2048, 8192, 32768],\n",
    "        w_sc=0.0,\n",
    "        w_phs=0.0,\n",
    "        w_lin_mag=1.0,\n",
    "        w_log_mag=1.0,\n",
    "    ),\n",
    "    \"MSD\": MelSpectralDistance(SAMPLE_RATE),\n",
    "    \"SCE\": SpectralCentroidError(SAMPLE_RATE),\n",
    "    \"CFE\": CrestFactorError(),\n",
    "    \"LUFS\": LoudnessError(SAMPLE_RATE),\n",
    "    \"RMS\": RMSEnergyError()\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "EndToEndSystem(\n  (audio_encoder): SpectrogramVAE(\n    (encoder_conv): Sequential(\n      (0): Sequential(\n        (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (1): ReLU()\n        (2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): Sequential(\n        (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (1): ReLU()\n        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): Sequential(\n        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (1): ReLU()\n        (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (3): Sequential(\n        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (1): ReLU()\n        (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (mu): Linear(in_features=37152, out_features=128, bias=True)\n    (log_var): Linear(in_features=37152, out_features=128, bias=True)\n    (decoder_linear): Sequential(\n      (0): Linear(in_features=128, out_features=37152, bias=True)\n      (1): ReLU()\n    )\n    (decoder_conv): Sequential(\n      (0): Sequential(\n        (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (1): ReLU()\n        (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): Sequential(\n        (0): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (1): ReLU()\n        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): Sequential(\n        (0): ConvTranspose2d(16, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (1): ReLU()\n        (2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (3): Sequential(\n        (0): ConvTranspose2d(8, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      )\n    )\n  )\n  (controller): Sequential(\n    (0): Linear(in_features=256, out_features=128, bias=True)\n    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n    (2): LeakyReLU(negative_slope=0.01)\n    (3): Linear(in_features=128, out_features=128, bias=True)\n    (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n    (5): LeakyReLU(negative_slope=0.01)\n    (6): Linear(in_features=128, out_features=64, bias=True)\n    (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n    (8): LeakyReLU(negative_slope=0.01)\n    (9): Linear(in_features=64, out_features=32, bias=True)\n    (10): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n    (11): LeakyReLU(negative_slope=0.01)\n    (12): Linear(in_features=32, out_features=3, bias=True)\n  )\n  (dafx_layer): DAFXLayer()\n  (recon_losses): ModuleDict(\n    (mrstft): MultiResolutionSTFTLoss(\n      (stft_losses): ModuleList(\n        (0): STFTLoss(\n          (spectralconv): SpectralConvergenceLoss()\n          (logstft): STFTMagnitudeLoss(\n            (distance): L1Loss()\n          )\n          (linstft): STFTMagnitudeLoss(\n            (distance): L1Loss()\n          )\n        )\n        (1): STFTLoss(\n          (spectralconv): SpectralConvergenceLoss()\n          (logstft): STFTMagnitudeLoss(\n            (distance): L1Loss()\n          )\n          (linstft): STFTMagnitudeLoss(\n            (distance): L1Loss()\n          )\n        )\n        (2): STFTLoss(\n          (spectralconv): SpectralConvergenceLoss()\n          (logstft): STFTMagnitudeLoss(\n            (distance): L1Loss()\n          )\n          (linstft): STFTMagnitudeLoss(\n            (distance): L1Loss()\n          )\n        )\n        (3): STFTLoss(\n          (spectralconv): SpectralConvergenceLoss()\n          (logstft): STFTMagnitudeLoss(\n            (distance): L1Loss()\n          )\n          (linstft): STFTMagnitudeLoss(\n            (distance): L1Loss()\n          )\n        )\n        (4): STFTLoss(\n          (spectralconv): SpectralConvergenceLoss()\n          (logstft): STFTMagnitudeLoss(\n            (distance): L1Loss()\n          )\n          (linstft): STFTMagnitudeLoss(\n            (distance): L1Loss()\n          )\n        )\n        (5): STFTLoss(\n          (spectralconv): SpectralConvergenceLoss()\n          (logstft): STFTMagnitudeLoss(\n            (distance): L1Loss()\n          )\n          (linstft): STFTMagnitudeLoss(\n            (distance): L1Loss()\n          )\n        )\n      )\n    )\n    (l1): L1Loss()\n  )\n)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "model = EndToEndSystem.load_from_checkpoint(CHECKPOINT).to(DEVICE)\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 9/9 [00:00<00:00, 14407.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 9 files for val = 0.38 hours.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "dafx = dafx_from_name(DAFX)\n",
    "\n",
    "dataset = PairedAudioDataset(\n",
    "    dafx=dafx,\n",
    "    audio_dir=AUDIO_DIR,\n",
    "    subset=\"val\",\n",
    "    train_frac=0.8,\n",
    "    input_dirs=DATASET_INPUT_DIRS,\n",
    "    num_examples_per_epoch=NUM_EXAMPLES,\n",
    "    augmentations={},\n",
    "    length=model.hparams.train_length * 2,\n",
    "    effect_input=False,\n",
    "    effect_output=True,\n",
    "    random_effect_threshold=0.,\n",
    "    dummy_setting=False\n",
    ")\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    num_workers=1,\n",
    "    batch_size=1,\n",
    "    timeout=6000,\n",
    "    shuffle=False,\n",
    "    generator=g\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:22<00:00, 44.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# get values\n",
    "outputs = []\n",
    "\n",
    "for batch in tqdm(loader):\n",
    "    x, y = batch\n",
    "    x, y_ref, y = get_training_reference(x, y)\n",
    "\n",
    "    y_hat, p, z = model(x.to(DEVICE), y=y_ref.to(DEVICE))\n",
    "\n",
    "    outputs.append({\n",
    "        \"y\": y.detach().cpu(),\n",
    "        \"y_hat\": y_hat.detach().cpu(),\n",
    "    })"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "results = {\n",
    "            \"PESQ\": [],\n",
    "            \"MRSTFT\": [],\n",
    "            \"MSD\": [],\n",
    "            \"SCE\": [],\n",
    "            \"CFE\": [],\n",
    "            \"LUFS\": [],\n",
    "            \"RMS\": [],\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 503/1000 [01:18<01:14,  6.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some error occurred:  b'No utterances detected'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:36<00:00,  6.40it/s]\n"
     ]
    }
   ],
   "source": [
    "for output in tqdm(outputs):\n",
    "    for metric_name, metric in metrics.items():\n",
    "        try:\n",
    "            val = metric(output[\"y_hat\"], output[\"y\"])\n",
    "            if type(val) == torch.Tensor:\n",
    "                val = val.numpy()\n",
    "            results[metric_name].append(val)\n",
    "        except Exception as e:\n",
    "            print(\"Some error occurred: \", e)\n",
    "            results[metric_name].append(np.NaN)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       PESQ     MRSTFT        MSD         SCE        CFE      LUFS       RMS\n",
      "0  2.955839  1.2800151  7.4726267    5.682373  10.355343  3.377809  8.342247\n",
      "1  4.505720  0.9200942  4.2486234  259.835449   2.124372  0.950178  2.357044\n",
      "2  3.639986   1.345643   5.394623  599.538269   1.662407  1.564906  4.213348\n",
      "3  4.253411  0.4026405  6.6475515  104.319763   1.042591  4.088740  9.854973\n",
      "4  4.379947  1.0551684  4.6367507  513.889954   0.028324  2.132826  5.311535\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "results_filename = get_results_filename(RESULTS_DIR, DAFX, DATASET)\n",
    "df.to_csv(results_filename)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
