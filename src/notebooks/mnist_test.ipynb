{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from math import prod\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from torchvision import datasets, transforms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MnistVAE(pl.LightningModule):\n",
    "    # =========== MAGIC METHODS =============\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Load instances for each type of DAFX\n",
    "        self.hidden_dim_enc = prod(self.hparams.hidden_dim)\n",
    "        self.hidden_dim_dec = self.hparams.hidden_dim\n",
    "\n",
    "        self._build_model()\n",
    "\n",
    "    # =========== PRIVATE METHODS =============\n",
    "    def _build_model(self):\n",
    "        self._build_encoder()\n",
    "        self._build_decoder()\n",
    "\n",
    "    def _build_encoder(self):\n",
    "        self.enc_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.hparams.num_channels,\n",
    "                      out_channels=8,\n",
    "                      kernel_size=self.hparams.conv_kernel,\n",
    "                      padding=self.hparams.conv_padding,\n",
    "                      stride=self.hparams.conv_stride\n",
    "                      ),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(8)\n",
    "        )\n",
    "\n",
    "        self.enc_conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8,\n",
    "                      out_channels=16,\n",
    "                      kernel_size=self.hparams.conv_kernel,\n",
    "                      padding=self.hparams.conv_padding,\n",
    "                      stride=self.hparams.conv_stride\n",
    "                      ),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "        )\n",
    "\n",
    "        self.enc_conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16,\n",
    "                      out_channels=32,\n",
    "                      kernel_size=self.hparams.conv_kernel,\n",
    "                      padding=self.hparams.conv_padding,\n",
    "                      stride=self.hparams.conv_stride\n",
    "                      ),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "\n",
    "        self.mu = nn.Linear(self.hidden_dim_enc, self.hparams.latent_dim)\n",
    "        self.log_var = nn.Linear(self.hidden_dim_enc, self.hparams.latent_dim)\n",
    "\n",
    "    def _build_decoder(self):\n",
    "        self.dec_hidden = nn.Sequential(\n",
    "            nn.Linear(in_features=self.hparams.latent_dim, out_features=self.hidden_dim_enc),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.dec_conv1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=32,\n",
    "                               out_channels=16,\n",
    "                               kernel_size=self.hparams.conv_kernel,\n",
    "                               padding=self.hparams.conv_padding,\n",
    "                               stride=self.hparams.conv_stride\n",
    "                               ),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16)\n",
    "        )\n",
    "\n",
    "        self.dec_conv2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=16,\n",
    "                               out_channels=8,\n",
    "                               kernel_size=self.hparams.conv_kernel,\n",
    "                               padding=self.hparams.conv_padding,\n",
    "                               stride=self.hparams.conv_stride\n",
    "                               ),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(8),\n",
    "        )\n",
    "\n",
    "        self.dec_conv3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=8,\n",
    "                               out_channels=self.hparams.num_channels,\n",
    "                               kernel_size=self.hparams.conv_kernel,\n",
    "                               padding=self.hparams.conv_padding,\n",
    "                               stride=self.hparams.conv_stride\n",
    "                               ),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_kl_loss(mu, log_var):\n",
    "        # calculate KL divergence\n",
    "        kld_batch = -0.5 * torch.sum(1 + log_var - torch.square(mu) - torch.exp(log_var), dim=1)\n",
    "        kld = torch.mean(kld_batch)\n",
    "\n",
    "        return kld\n",
    "\n",
    "    def _calculate_reconstruction_loss(self, x, x_hat):\n",
    "        if self.hparams.recon_loss.lower() == \"mse\":\n",
    "            return F.mse_loss(x, x_hat, reduction=\"mean\")\n",
    "        elif self.hparams.recon_loss.lower() == \"l1\":\n",
    "            return F.l1_loss(x, x_hat, reduction=\"mean\")\n",
    "        elif self.hparams.recon_loss.lower() == \"bce\":\n",
    "            return F.binary_cross_entropy(x, x_hat, reduction=\"mean\")\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.enc_conv1(x)\n",
    "        x = self.enc_conv2(x)\n",
    "        x = self.enc_conv3(x)\n",
    "\n",
    "        x = x.view(-1, self.hidden_dim_enc)\n",
    "\n",
    "        mu = self.mu(x)\n",
    "        log_var = self.log_var(x)\n",
    "\n",
    "        return mu, log_var\n",
    "\n",
    "    def decode(self, z):\n",
    "        x = self.dec_hidden(z)\n",
    "\n",
    "        x = x.view(-1, *self.hidden_dim_dec)\n",
    "\n",
    "        x = self.dec_conv1(x)\n",
    "        x = self.dec_conv2(x)\n",
    "        x = self.dec_conv3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def reparameterise(mu, log_var):\n",
    "        std = torch.exp(log_var / 2)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterise(mu, log_var)\n",
    "        out = self.decode(z)\n",
    "\n",
    "        return out, mu, log_var\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "    def common_paired_step(\n",
    "            self,\n",
    "            batch: Tuple,\n",
    "            batch_idx: int,\n",
    "            train: bool = False,\n",
    "    ):\n",
    "        # Get spectrograms\n",
    "        x, _ = batch\n",
    "\n",
    "        # Get reconstruction as well as mu, var\n",
    "        x_hat, x_mu, x_log_var = self(x)\n",
    "\n",
    "        # Calculate recon losses for clean/effected signals\n",
    "        recon_loss = self._calculate_reconstruction_loss(x, x_hat)\n",
    "        kld = self._calculate_kl_loss(x_mu, x_log_var)\n",
    "\n",
    "        # Total loss is additive\n",
    "        loss = recon_loss + (self.hparams.vae_beta * kld)\n",
    "\n",
    "        # log the losses\n",
    "        self.log((\"train\" if train else \"val\") + \"_loss/loss\", loss)\n",
    "        self.log((\"train\" if train else \"val\") + \"_loss/reconstruction_loss\", recon_loss)\n",
    "        self.log((\"train\" if train else \"val\") + \"_loss/kl_divergence\", kld)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.common_paired_step(\n",
    "            batch,\n",
    "            batch_idx,\n",
    "            train=True,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.common_paired_step(\n",
    "            batch,\n",
    "            batch_idx,\n",
    "            train=False,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "\n",
    "        # -------- Training -----------\n",
    "        parser.add_argument(\"--batch_size\", type=int, default=8)\n",
    "        parser.add_argument(\"--lr\", type=float, default=1e-5)\n",
    "        parser.add_argument(\"--recon_loss\", type=str, default=\"mse\")\n",
    "        parser.add_argument(\"--vae_beta\", type=float, default=1.)\n",
    "\n",
    "        # --------- DAFX ------------\n",
    "        parser.add_argument(\"--dafx_file\", type=str, default=\"src/dafx/mda.vst3\")\n",
    "        parser.add_argument(\"--dafx_names\", nargs=\"*\")\n",
    "        parser.add_argument(\"--dafx_param_names\", nargs=\"*\", default=None)\n",
    "\n",
    "        # --------- VAE -------------\n",
    "        parser.add_argument(\"--num_channels\", type=int, default=1)\n",
    "        parser.add_argument(\"--hidden_dim\", nargs=\"*\", default=(32, 9, 257))\n",
    "        parser.add_argument(\"--latent_dim\", type=int, default=1024)\n",
    "        parser.add_argument(\"--conv_kernel\", type=int, default=3)\n",
    "        parser.add_argument(\"--conv_padding\", type=int, default=1)\n",
    "        parser.add_argument(\"--conv_stride\", type=int, default=2)\n",
    "\n",
    "        # ------- Dataset  -----------\n",
    "        parser.add_argument(\"--audio_dir\", type=str, default=\"src/audio\")\n",
    "        parser.add_argument(\"--ext\", type=str, default=\"wav\")\n",
    "        parser.add_argument(\"--input_dirs\", nargs=\"+\", default=['musdb18_24000', 'vctk_24000'])\n",
    "        parser.add_argument(\"--buffer_reload_rate\", type=int, default=1000)\n",
    "        parser.add_argument(\"--buffer_size_gb\", type=float, default=1.0)\n",
    "        parser.add_argument(\"--sample_rate\", type=int, default=24_000)\n",
    "        parser.add_argument(\"--dsp_sample_rate\", type=int, default=24_000)\n",
    "        parser.add_argument(\"--shuffle\", type=bool, default=True)\n",
    "        parser.add_argument(\"--random_effect_threshold\", type=float, default=0.75)\n",
    "        parser.add_argument(\"--train_length\", type=int, default=131_072)\n",
    "        parser.add_argument(\"--train_frac\", type=float, default=0.9)\n",
    "        parser.add_argument(\"--effect_audio\", type=bool, default=True)\n",
    "        parser.add_argument(\"--half\", type=bool, default=False)\n",
    "        parser.add_argument(\"--train_examples_per_epoch\", type=int, default=10_000)\n",
    "        parser.add_argument(\"--val_length\", type=int, default=131_072)\n",
    "        parser.add_argument(\"--val_examples_per_epoch\", type=int, default=100)\n",
    "        parser.add_argument(\"--num_workers\", type=int, default=4)\n",
    "        parser.add_argument(\"--dummy_setting\", type=bool, default=False)\n",
    "\n",
    "        return parser\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# prerequisites\n",
    "\n",
    "\n",
    "bs = 100\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False, num_workers=4)\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"conv_kernel\": 3,\n",
    "    \"conv_padding\": 1,\n",
    "    \"conv_stride\": 1,\n",
    "    \"latent_dim\": 256,\n",
    "    \"hidden_dim\": [32,28,28],\n",
    "    \"num_channels\": 1,\n",
    "    \"dafx_names\": [],\n",
    "    \"lr\": 1e-4,\n",
    "    \"recon_loss\": \"mse\",\n",
    "    \"vae_beta\":1,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = MnistVAE(**args)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss/loss\", mode=\"min\")\n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", max_epochs=100, callbacks=[checkpoint_callback])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.fit(model, train_loader, test_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False, num_workers=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NUM_EXAMPLES=5\n",
    "\n",
    "fig, axes = plt.subplots(NUM_EXAMPLES, 2, figsize=(8,5))\n",
    "\n",
    "for i, spc in enumerate(test_loader):\n",
    "    if i >= NUM_EXAMPLES:\n",
    "        break\n",
    "\n",
    "    x, _ = spc\n",
    "    x_hat, _, _ = model(x)\n",
    "\n",
    "    mse = F.mse_loss(x, x_hat)\n",
    "\n",
    "    ax1 = axes[i, 0].imshow(x.squeeze().numpy())\n",
    "    ax2 = axes[i, 1].imshow(x_hat.detach().squeeze().numpy())\n",
    "\n",
    "    axes[i, 0].set_title(f\"Original {i+1}\", fontsize=10)\n",
    "    axes[i, 1].set_title(f\"Reconstruction {i+1} (MSE: {mse.item():.4f})\", fontsize=10)\n",
    "\n",
    "    axes[i, 0].set_xticks([])\n",
    "    axes[i, 0].set_yticks([])\n",
    "\n",
    "    axes[i, 1].set_xticks([])\n",
    "    axes[i, 1].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./figures/mnist_reconstruction.svg\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
